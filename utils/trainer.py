"""
è®­ç»ƒå™¨æ¨¡å—
è´Ÿè´£äºº: B3
åŠŸèƒ½: å°è£…è®­ç»ƒå¾ªç¯ï¼Œæ”¯æŒMixUpã€å­¦ä¹ ç‡è°ƒåº¦ã€checkpointä¿å­˜
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import Optimizer
from torch.optim.lr_scheduler import _LRScheduler
from typing import Optional, Dict, Tuple
import os
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter


class Trainer:
    """
    è®­ç»ƒå™¨ç±»
    
    åŠŸèƒ½:
    - è®­ç»ƒå¾ªç¯
    - éªŒè¯å¾ªç¯
    - å­¦ä¹ ç‡è°ƒåº¦
    - Checkpointä¿å­˜
    - TensorBoardæ—¥å¿—
    - MixUpæ”¯æŒ
    """
    
    def __init__(
        self,
        model: nn.Module,
        criterion: nn.Module,
        optimizer: Optimizer,
        scheduler: Optional[_LRScheduler],
        device: str = 'cuda',
        save_dir: str = '../outputs/models',
        use_tensorboard: bool = True,
        log_dir: str = '../outputs/logs',
        use_mixup: bool = False,
        mixup_alpha: float = 1.0
    ):
        """
        Args:
            model: æ¨¡å‹
            criterion: æŸå¤±å‡½æ•°
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            device: è®­ç»ƒè®¾å¤‡
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            use_tensorboard: æ˜¯å¦ä½¿ç”¨TensorBoard
            log_dir: TensorBoardæ—¥å¿—ç›®å½•
            use_mixup: æ˜¯å¦ä½¿ç”¨MixUp
            mixup_alpha: MixUpçš„alphaå‚æ•°
        """
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.save_dir = save_dir
        self.use_mixup = use_mixup
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # TensorBoard
        self.writer = None
        if use_tensorboard:
            os.makedirs(log_dir, exist_ok=True)
            self.writer = SummaryWriter(log_dir)
            print(f"ğŸ“Š TensorBoardæ—¥å¿—: {log_dir}")
        
        # MixUp
        if use_mixup:
            from data.augmentation import MixUpAugmentation
            self.mixup = MixUpAugmentation(alpha=mixup_alpha)
            print(f"âœ… å¯ç”¨MixUp (alpha={mixup_alpha})")
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'lr': []
        }
        
        self.best_val_acc = 0.0
        self.current_epoch = 0
    
    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Tuple[float, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epochæ•°
        
        Returns:
            avg_loss: å¹³å‡æŸå¤±
            avg_acc: å¹³å‡å‡†ç¡®ç‡
        """
        self.model.train()
        
        running_loss = 0.0
        correct = 0
        total = 0
        
        # è¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
        
        for batch_idx, (images, labels) in enumerate(pbar):
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            # MixUp
            if self.use_mixup:
                images, labels_a, labels_b, lam = self.mixup(images, labels)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(images)
                
                # MixUpæŸå¤±
                loss = lam * self.criterion(outputs, labels_a) + \
                       (1 - lam) * self.criterion(outputs, labels_b)
                
                # å‡†ç¡®ç‡ï¼ˆä½¿ç”¨åŸå§‹æ ‡ç­¾ï¼‰
                _, predicted = outputs.max(1)
                correct += (lam * predicted.eq(labels_a).sum().item() + \
                           (1 - lam) * predicted.eq(labels_b).sum().item())
            
            else:
                # æ ‡å‡†è®­ç»ƒ
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                # å‡†ç¡®ç‡
                _, predicted = outputs.max(1)
                correct += predicted.eq(labels).sum().item()
            
            total += labels.size(0)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_loss = running_loss / (batch_idx + 1)
            avg_acc = 100. * correct / total
            pbar.set_postfix({
                'loss': f'{avg_loss:.4f}',
                'acc': f'{avg_acc:.2f}%',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
        
        avg_loss = running_loss / len(train_loader)
        avg_acc = 100. * correct / total
        
        return avg_loss, avg_acc
    
    def validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """
        éªŒè¯
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        
        Returns:
            avg_loss: å¹³å‡æŸå¤±
            avg_acc: å¹³å‡å‡†ç¡®ç‡
        """
        self.model.eval()
        
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc='Validating'):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                _, predicted = outputs.max(1)
                correct += predicted.eq(labels).sum().item()
                total += labels.size(0)
                
                running_loss += loss.item()
        
        avg_loss = running_loss / len(val_loader)
        avg_acc = 100. * correct / total
        
        return avg_loss, avg_acc
    
    def fit(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        num_epochs: int,
        save_best_only: bool = True,
        save_frequency: int = 5
    ):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            save_best_only: æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹
            save_frequency: ä¿å­˜é¢‘ç‡ï¼ˆæ¯Nä¸ªepochï¼‰
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ")
        print(f"   æ€»epochæ•°: {num_epochs}")
        print(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
        print(f"   éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"{'='*60}\n")
        
        for epoch in range(1, num_epochs + 1):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                self.scheduler.step()
            
            # è®°å½•å†å²
            current_lr = self.optimizer.param_groups[0]['lr']
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['lr'].append(current_lr)
            
            # TensorBoard
            if self.writer:
                self.writer.add_scalar('Loss/train', train_loss, epoch)
                self.writer.add_scalar('Loss/val', val_loss, epoch)
                self.writer.add_scalar('Accuracy/train', train_acc, epoch)
                self.writer.add_scalar('Accuracy/val', val_acc, epoch)
                self.writer.add_scalar('LearningRate', current_lr, epoch)
            
            # æ‰“å°ç»“æœ
            print(f"\nğŸ“Š Epoch {epoch}/{num_epochs} ç»“æœ:")
            print(f"   è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"   éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
            print(f"   å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.save_checkpoint('best_model.pth', is_best=True)
                print(f"   â­ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            
            # å®šæœŸä¿å­˜
            if not save_best_only and epoch % save_frequency == 0:
                self.save_checkpoint(f'epoch_{epoch}.pth')
            
            print()
        
        print(f"{'='*60}")
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%")
        print(f"{'='*60}\n")
        
        if self.writer:
            self.writer.close()
    
    def save_checkpoint(self, filename: str, is_best: bool = False):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        
        Args:
            filename: æ–‡ä»¶å
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_acc': self.best_val_acc,
            'history': self.history
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        filepath = os.path.join(self.save_dir, filename)
        torch.save(checkpoint, filepath)
        
        if is_best:
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """
        åŠ è½½æ£€æŸ¥ç‚¹
        
        Args:
            filepath: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        """
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.current_epoch = checkpoint['epoch']
        self.best_val_acc = checkpoint['best_val_acc']
        self.history = checkpoint['history']
        
        print(f"âœ… åŠ è½½æ£€æŸ¥ç‚¹: {filepath}")
        print(f"   Epoch: {self.current_epoch}, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%")


if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒå™¨
    print("è®­ç»ƒå™¨æ¨¡å—å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä½†éœ€è¦å®Œæ•´çš„æ•°æ®å’Œæ¨¡å‹")
    print("è¯·åœ¨main.pyä¸­ä½¿ç”¨")