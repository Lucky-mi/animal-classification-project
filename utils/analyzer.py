"""
é”™è¯¯æ ·æœ¬åˆ†ææ¨¡å—
è´Ÿè´£äºº: B3
åŠŸèƒ½: æ·±åº¦åˆ†æè¢«é”™åˆ†çš„æ ·æœ¬ï¼Œè¿™æ˜¯å®éªŒ6çš„æ ¸å¿ƒ â­
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import os
from collections import defaultdict
from PIL import Image


class ErrorAnalyzer:
    """
    é”™è¯¯æ ·æœ¬åˆ†æå™¨
    
    åŠŸèƒ½:
    1. ç»Ÿè®¡æ··æ·†å¯¹ï¼ˆå“ªä¸¤ä¸ªç±»åˆ«æœ€å®¹æ˜“æ··æ·†ï¼‰
    2. ä¿å­˜è¢«é”™åˆ†çš„æ ·æœ¬
    3. åˆ†æé”™è¯¯æ¨¡å¼
    4. ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š
    """
    
    def __init__(
        self,
        class_names: List[str],
        save_dir: str = '../outputs/error_analysis'
    ):
        """
        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•
        """
        self.class_names = class_names
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
    
    def analyze_confusion_pairs(
        self,
        conf_matrix: np.ndarray,
        top_k: int = 5
    ) -> List[Tuple[str, str, int]]:
        """
        åˆ†ææœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹
        
        Args:
            conf_matrix: æ··æ·†çŸ©é˜µ [num_classes, num_classes]
            top_k: è¿”å›å‰Kä¸ªæ··æ·†å¯¹
        
        Returns:
            confusion_pairs: [(çœŸå®ç±»åˆ«, é¢„æµ‹ç±»åˆ«, é”™è¯¯æ•°é‡)]
        """
        num_classes = len(self.class_names)
        confusion_pairs = []
        
        # éå†éå¯¹è§’çº¿å…ƒç´ 
        for i in range(num_classes):
            for j in range(num_classes):
                if i != j:  # è·³è¿‡å¯¹è§’çº¿
                    count = conf_matrix[i, j]
                    if count > 0:
                        confusion_pairs.append((
                            self.class_names[i],
                            self.class_names[j],
                            int(count)
                        ))
        
        # æŒ‰é”™è¯¯æ•°é‡æ’åº
        confusion_pairs.sort(key=lambda x: x[2], reverse=True)
        
        return confusion_pairs[:top_k]
    
    def save_misclassified_samples(
        self,
        misclassified: Dict,
        dataset,
        max_samples_per_pair: int = 10
    ):
        """
        ä¿å­˜è¢«é”™åˆ†çš„æ ·æœ¬
        
        Args:
            misclassified: {(true_label, pred_label): [sample_info]}
            dataset: æ•°æ®é›†å¯¹è±¡
            max_samples_per_pair: æ¯å¯¹æœ€å¤šä¿å­˜å¤šå°‘æ ·æœ¬
        """
        print(f"\nğŸ’¾ ä¿å­˜é”™è¯¯æ ·æœ¬åˆ°: {self.save_dir}")
        
        for (true_idx, pred_idx), samples in misclassified.items():
            true_name = self.class_names[true_idx]
            pred_name = self.class_names[pred_idx]
            
            # åˆ›å»ºå­ç›®å½•
            pair_dir = os.path.join(self.save_dir, f'{true_name}_as_{pred_name}')
            os.makedirs(pair_dir, exist_ok=True)
            
            # ä¿å­˜æ ·æœ¬
            num_saved = min(len(samples), max_samples_per_pair)
            
            for i, sample in enumerate(samples[:num_saved]):
                img_tensor = sample['image']
                confidence = sample['confidence']
                
                # åå½’ä¸€åŒ–
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                img_tensor = img_tensor * std + mean
                img_tensor = torch.clamp(img_tensor, 0, 1)
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                img_np = img_tensor.permute(1, 2, 0).numpy()
                img_pil = Image.fromarray((img_np * 255).astype(np.uint8))
                
                # ä¿å­˜
                filename = f'sample_{i+1}_conf_{confidence:.3f}.jpg'
                img_pil.save(os.path.join(pair_dir, filename))
            
            print(f"   {true_name} â†’ {pred_name}: ä¿å­˜äº† {num_saved}/{len(samples)} ä¸ªæ ·æœ¬")
    
    def visualize_error_samples(
        self,
        misclassified: Dict,
        top_k_pairs: int = 3,
        samples_per_pair: int = 6
    ):
        """
        å¯è§†åŒ–é”™è¯¯æ ·æœ¬
        
        Args:
            misclassified: {(true_label, pred_label): [sample_info]}
            top_k_pairs: å±•ç¤ºå‰Kä¸ªæ··æ·†å¯¹
            samples_per_pair: æ¯å¯¹å±•ç¤ºå¤šå°‘æ ·æœ¬
        """
        # æŒ‰é”™è¯¯æ•°é‡æ’åº
        sorted_pairs = sorted(
            misclassified.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )[:top_k_pairs]
        
        for pair_idx, ((true_idx, pred_idx), samples) in enumerate(sorted_pairs):
            true_name = self.class_names[true_idx]
            pred_name = self.class_names[pred_idx]
            
            num_samples = min(len(samples), samples_per_pair)
            
            fig, axes = plt.subplots(1, num_samples, figsize=(3 * num_samples, 3))
            if num_samples == 1:
                axes = [axes]
            
            fig.suptitle(
                f'é”™è¯¯æ ·æœ¬: {true_name} è¢«é¢„æµ‹ä¸º {pred_name} (å…±{len(samples)}ä¸ª)',
                fontsize=14
            )
            
            for i in range(num_samples):
                sample = samples[i]
                img_tensor = sample['image']
                confidence = sample['confidence']
                
                # åå½’ä¸€åŒ–
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                img_tensor = img_tensor * std + mean
                img_tensor = torch.clamp(img_tensor, 0, 1)
                
                img_np = img_tensor.permute(1, 2, 0).numpy()
                
                axes[i].imshow(img_np)
                axes[i].set_title(f'ç½®ä¿¡åº¦: {confidence:.2%}', fontsize=10)
                axes[i].axis('off')
            
            plt.tight_layout()
            save_path = os.path.join(self.save_dir, f'error_pair_{pair_idx+1}_{true_name}_as_{pred_name}.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ é”™è¯¯æ ·æœ¬å¯è§†åŒ–: {save_path}")
            plt.close()
    
    def generate_report(
        self,
        conf_matrix: np.ndarray,
        misclassified: Dict,
        save_path: str = None
    ):
        """
        ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š
        
        Args:
            conf_matrix: æ··æ·†çŸ©é˜µ
            misclassified: é”™è¯¯æ ·æœ¬å­—å…¸
            save_path: æŠ¥å‘Šä¿å­˜è·¯å¾„
        """
        if save_path is None:
            save_path = os.path.join(self.save_dir, 'error_analysis_report.txt')
        
        # åˆ†ææ··æ·†å¯¹
        top_confusion = self.analyze_confusion_pairs(conf_matrix, top_k=10)
        
        # ç»Ÿè®¡æ€»é”™è¯¯æ•°
        total_errors = sum(len(samples) for samples in misclassified.values())
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("=" * 70)
        report.append("é”™è¯¯æ ·æœ¬åˆ†ææŠ¥å‘Š")
        report.append("=" * 70)
        report.append("")
        
        report.append(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        report.append(f"   æ€»é”™è¯¯æ ·æœ¬æ•°: {total_errors}")
        report.append(f"   é”™è¯¯ç±»å‹æ•°: {len(misclassified)}")
        report.append("")
        
        report.append(f"ğŸ” Top-10 æœ€å¸¸è§çš„æ··æ·†:")
        report.append(f"{'æ’å':<6} {'çœŸå®ç±»åˆ«':<12} {'é¢„æµ‹ä¸º':<12} {'é”™è¯¯æ•°é‡':<10}")
        report.append("-" * 70)
        
        for rank, (true_cls, pred_cls, count) in enumerate(top_confusion, 1):
            report.append(f"{rank:<6} {true_cls:<12} {pred_cls:<12} {count:<10}")
        
        report.append("")
        report.append("=" * 70)
        report.append("")
        
        report.append("ğŸ’¡ åˆ†æå»ºè®®:")
        report.append("")
        
        # è‡ªåŠ¨ç”Ÿæˆåˆ†æå»ºè®®
        if len(top_confusion) > 0:
            top1_true, top1_pred, top1_count = top_confusion[0]
            report.append(f"1. æœ€ä¸¥é‡çš„æ··æ·†: {top1_true} â†” {top1_pred} ({top1_count}ä¸ªæ ·æœ¬)")
            report.append(f"   å»ºè®®: æ£€æŸ¥è¿™ä¸¤ä¸ªç±»åˆ«çš„è§†è§‰ç›¸ä¼¼æ€§ï¼Œè€ƒè™‘:")
            report.append(f"   - ä½¿ç”¨Grad-CAMæŸ¥çœ‹æ¨¡å‹å…³æ³¨çš„åŒºåŸŸ")
            report.append(f"   - å¢åŠ é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼º")
            report.append(f"   - æ”¶é›†æ›´å¤šåŒºåˆ†æ€§æ ·æœ¬")
            report.append("")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æŸä¸ªç±»åˆ«ç‰¹åˆ«å®¹æ˜“è¢«è¯¯åˆ†
        per_class_errors = defaultdict(int)
        for (true_idx, pred_idx), samples in misclassified.items():
            per_class_errors[true_idx] += len(samples)
        
        if per_class_errors:
            worst_class_idx = max(per_class_errors, key=per_class_errors.get)
            worst_class_name = self.class_names[worst_class_idx]
            worst_count = per_class_errors[worst_class_idx]
            
            report.append(f"2. æœ€éš¾è¯†åˆ«çš„ç±»åˆ«: {worst_class_name} (è¢«è¯¯åˆ†{worst_count}æ¬¡)")
            report.append(f"   å»ºè®®: è¿™ä¸ªç±»åˆ«å¯èƒ½æ˜¯:")
            report.append(f"   - æ ·æœ¬è´¨é‡è¾ƒå·®ï¼ˆæ¨¡ç³Šã€é®æŒ¡ã€å…‰ç…§ä¸è¶³ï¼‰")
            report.append(f"   - ç±»å†…å·®å¼‚å¤§ï¼ˆä¸åŒè§†è§’ã€å§¿æ€ï¼‰")
            report.append(f"   - ä¸å…¶ä»–ç±»åˆ«è§†è§‰ç›¸ä¼¼")
            report.append("")
        
        report.append("=" * 70)
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report)
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"ğŸ“„ é”™è¯¯åˆ†ææŠ¥å‘Šä¿å­˜è‡³: {save_path}")
        print("\n" + report_text)


if __name__ == "__main__":
    print("é”™è¯¯åˆ†ææ¨¡å—æµ‹è¯•éœ€è¦å®Œæ•´çš„è¯„ä¼°ç»“æœ")
    print("è¯·åœ¨å®éªŒè„šæœ¬ä¸­ä½¿ç”¨")