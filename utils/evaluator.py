"""
è¯„ä¼°å™¨æ¨¡å—
è´Ÿè´£äºº: B3
åŠŸèƒ½: æ¨¡å‹è¯„ä¼°ï¼Œè®¡ç®—å¤šç§æŒ‡æ ‡ï¼ˆAccuracy, Macro F1, Per-class Accuracyï¼‰
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import Dict, List, Tuple
import numpy as np
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report
)
from tqdm import tqdm


class Evaluator:
    """
    è¯„ä¼°å™¨ç±»
    
    åŠŸèƒ½:
    - è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    - è®¡ç®—Macro F1
    - è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡/å¬å›ç‡/F1
    - ç”Ÿæˆæ··æ·†çŸ©é˜µ
    - æ”¶é›†é”™è¯¯æ ·æœ¬
    """
    
    def __init__(
        self,
        model: nn.Module,
        device: str = 'cuda',
        class_names: List[str] = None
    ):
        """
        Args:
            model: è¦è¯„ä¼°çš„æ¨¡å‹
            device: è®¾å¤‡
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
        """
        self.model = model
        self.device = device
        self.class_names = class_names
    
    def evaluate(
        self,
        data_loader: DataLoader,
        return_predictions: bool = False
    ) -> Dict:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            return_predictions: æ˜¯å¦è¿”å›é¢„æµ‹ç»“æœ
        
        Returns:
            results: è¯„ä¼°ç»“æœå­—å…¸
        """
        self.model.eval()
        
        all_labels = []
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for images, labels in tqdm(data_loader, desc='Evaluating'):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                probs = torch.softmax(outputs, dim=1)
                _, predicted = outputs.max(1)
                
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(predicted.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        all_labels = np.array(all_labels)
        all_preds = np.array(all_preds)
        all_probs = np.array(all_probs)
        
        # è®¡ç®—æŒ‡æ ‡
        results = self.calculate_metrics(all_labels, all_preds, all_probs)
        
        # å¦‚æœéœ€è¦è¿”å›é¢„æµ‹ç»“æœ
        if return_predictions:
            results['predictions'] = all_preds
            results['probabilities'] = all_probs
            results['labels'] = all_labels
        
        return results
    
    def calculate_metrics(
        self,
        labels: np.ndarray,
        preds: np.ndarray,
        probs: np.ndarray
    ) -> Dict:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Args:
            labels: çœŸå®æ ‡ç­¾ [N]
            preds: é¢„æµ‹æ ‡ç­¾ [N]
            probs: é¢„æµ‹æ¦‚ç‡ [N, num_classes]
        
        Returns:
            metrics: æŒ‡æ ‡å­—å…¸
        """
        # æ•´ä½“å‡†ç¡®ç‡
        accuracy = accuracy_score(labels, preds) * 100
        
        # æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
        precision, recall, f1, support = precision_recall_fscore_support(
            labels, preds, average=None, zero_division=0
        )
        
        # Macroå¹³å‡
        macro_precision = precision.mean() * 100
        macro_recall = recall.mean() * 100
        macro_f1 = f1.mean() * 100
        
        # æ··æ·†çŸ©é˜µ
        conf_matrix = confusion_matrix(labels, preds)
        
        # æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        per_class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1) * 100
        
        # ç»„ç»‡ç»“æœ
        results = {
            'accuracy': accuracy,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'confusion_matrix': conf_matrix,
            'per_class_metrics': {
                'precision': precision * 100,
                'recall': recall * 100,
                'f1': f1 * 100,
                'accuracy': per_class_acc,
                'support': support
            }
        }
        
        return results
    
    def print_results(self, results: Dict):
        """
        æ‰“å°è¯„ä¼°ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\n{'='*70}")
        print("ğŸ“Š è¯„ä¼°ç»“æœ")
        print(f"{'='*70}\n")
        
        # æ•´ä½“æŒ‡æ ‡
        print("ğŸ¯ æ•´ä½“æŒ‡æ ‡:")
        print(f"   å‡†ç¡®ç‡ (Accuracy): {results['accuracy']:.2f}%")
        print(f"   Macro ç²¾ç¡®ç‡: {results['macro_precision']:.2f}%")
        print(f"   Macro å¬å›ç‡: {results['macro_recall']:.2f}%")
        print(f"   Macro F1: {results['macro_f1']:.2f}%")
        
        # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        print(f"\nğŸ“‹ å„ç±»åˆ«æŒ‡æ ‡:")
        print(f"{'ç±»åˆ«':<12} {'ç²¾ç¡®ç‡':>8} {'å¬å›ç‡':>8} {'F1':>8} {'å‡†ç¡®ç‡':>8} {'æ ·æœ¬æ•°':>8}")
        print("-" * 70)
        
        per_class = results['per_class_metrics']
        
        for i, class_name in enumerate(self.class_names):
            print(f"{class_name:<12} "
                  f"{per_class['precision'][i]:>7.2f}% "
                  f"{per_class['recall'][i]:>7.2f}% "
                  f"{per_class['f1'][i]:>7.2f}% "
                  f"{per_class['accuracy'][i]:>7.2f}% "
                  f"{per_class['support'][i]:>8}")
        
        print(f"{'='*70}\n")
    
    def get_misclassified_samples(
        self,
        data_loader: DataLoader
    ) -> Dict:
        """
        è·å–æ‰€æœ‰è¢«é”™åˆ†çš„æ ·æœ¬
        
        Returns:
            misclassified: å­—å…¸ï¼Œé”®ä¸º(true_label, pred_label)ï¼Œå€¼ä¸ºæ ·æœ¬ç´¢å¼•åˆ—è¡¨
        """
        self.model.eval()
        
        misclassified = {}
        sample_idx = 0
        
        with torch.no_grad():
            for images, labels in tqdm(data_loader, desc='Finding errors'):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                _, predicted = outputs.max(1)
                
                # æ‰¾åˆ°é”™è¯¯æ ·æœ¬
                mask = predicted != labels
                
                for i in range(len(labels)):
                    if mask[i]:
                        true_label = labels[i].item()
                        pred_label = predicted[i].item()
                        key = (true_label, pred_label)
                        
                        if key not in misclassified:
                            misclassified[key] = []
                        
                        misclassified[key].append({
                            'index': sample_idx + i,
                            'image': images[i].cpu(),
                            'true_label': true_label,
                            'pred_label': pred_label,
                            'confidence': torch.softmax(outputs[i], dim=0)[pred_label].item()
                        })
                
                sample_idx += len(labels)
        
        return misclassified


if __name__ == "__main__":
    print("è¯„ä¼°å™¨æ¨¡å—æµ‹è¯•éœ€è¦å®Œæ•´çš„æ¨¡å‹å’Œæ•°æ®")
    print("è¯·åœ¨å®éªŒè„šæœ¬ä¸­ä½¿ç”¨")