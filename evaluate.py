"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
åŠŸèƒ½: è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
ä½¿ç”¨æ–¹æ³•:
    python evaluate.py --checkpoint ../outputs/exp1_baseline/models/best_model.pth --config configs/exp1_baseline.yaml
"""

import os
import yaml
import argparse
import torch
import numpy as np

from data import get_dataloaders, get_val_transform
from models import get_model
from utils import Evaluator, plot_confusion_matrix, ErrorAnalyzer


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main(args):
    """ä¸»è¯„ä¼°æµç¨‹"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š æ¨¡å‹è¯„ä¼°")
    print(f"{'='*70}\n")
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\n")
    
    # åŠ è½½æ•°æ®
    val_transform = get_val_transform(image_size=config['data']['image_size'])
    
    _, val_loader, test_loader, class_to_idx = get_dataloaders(
        root_dir=config['data']['root_dir'],
        csv_file=config['data']['csv_file'],
        train_transform=val_transform,  # è¯„ä¼°æ—¶ä¸éœ€è¦è®­ç»ƒå¢å¼º
        val_transform=val_transform,
        batch_size=args.batch_size,
        num_workers=4,
        pin_memory=True
    )
    
    class_names = config['data']['classes']
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {config['model']['name']}")
    model = get_model(
        model_name=config['model']['name'],
        num_classes=config['data']['num_classes'],
        pretrained=False,  # è¯„ä¼°æ—¶ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡
        dropout=config['model']['dropout'],
        device=device
    )
    
    # åŠ è½½checkpoint
    print(f"ğŸ“¥ åŠ è½½checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"   è®­ç»ƒepoch: {checkpoint['epoch']}")
    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint['best_val_acc']:.2f}%\n")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Evaluator(
        model=model,
        device=device,
        class_names=class_names
    )
    
    # é€‰æ‹©è¯„ä¼°é›†
    if args.split == 'val':
        eval_loader = val_loader
        print("ğŸ“Š è¯„ä¼°éªŒè¯é›†...")
    else:
        eval_loader = test_loader
        print("ğŸ“Š è¯„ä¼°æµ‹è¯•é›†...")
    
    # è¯„ä¼°
    results = evaluator.evaluate(eval_loader, return_predictions=True)
    
    # æ‰“å°ç»“æœ
    evaluator.print_results(results)
    
 # åˆ›å»ºè¾“å‡ºç›®å½•
    # ä¼˜å…ˆä»checkpointè·¯å¾„æ¨æ–­exp_name: ../outputs/exp_name/models/best_model.pth
    checkpoint_abs = os.path.abspath(args.checkpoint)
    checkpoint_dir = os.path.dirname(os.path.dirname(checkpoint_abs))
    exp_name = os.path.basename(checkpoint_dir)

      # å¦‚æœæ¨æ–­å¤±è´¥ï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶å
    if exp_name in ['outputs', '..', '', 'project']:
        if 'exp_name' in config:
            exp_name = config['exp_name']
        else:
            exp_name = os.path.splitext(os.path.basename(args.config))[0]

    output_dir = os.path.join('../outputs', exp_name, 'evaluation')
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ··æ·†çŸ©é˜µ
    conf_matrix_path = os.path.join(output_dir, f'confusion_matrix_{args.split}.png')
    plot_confusion_matrix(
        conf_matrix=results['confusion_matrix'],
        class_names=class_names,
        save_path=conf_matrix_path,
        normalize=False
    )
    
    # ä¿å­˜å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    conf_matrix_norm_path = os.path.join(output_dir, f'confusion_matrix_{args.split}_normalized.png')
    plot_confusion_matrix(
        conf_matrix=results['confusion_matrix'],
        class_names=class_names,
        save_path=conf_matrix_norm_path,
        normalize=True
    )
    
    # é”™è¯¯æ ·æœ¬åˆ†æ
    if args.analyze_errors:
        print(f"\n{'='*70}")
        print("ğŸ” é”™è¯¯æ ·æœ¬åˆ†æ")
        print(f"{'='*70}\n")
        
        analyzer = ErrorAnalyzer(
            class_names=class_names,
            save_dir=os.path.join(output_dir, 'error_analysis')
        )
        
        # è·å–é”™è¯¯æ ·æœ¬
        misclassified = evaluator.get_misclassified_samples(eval_loader)
        
        # ä¿å­˜é”™è¯¯æ ·æœ¬
        from data.dataset import AnimalDataset
        eval_dataset = AnimalDataset(
            root_dir=config['data']['root_dir'],
            csv_file=config['data']['csv_file'],
            split=args.split,
            transform=val_transform,
            class_to_idx=class_to_idx
        )
        
        analyzer.save_misclassified_samples(
            misclassified=misclassified,
            dataset=eval_dataset,
            max_samples_per_pair=10
        )
        
        # å¯è§†åŒ–é”™è¯¯æ ·æœ¬
        analyzer.visualize_error_samples(
            misclassified=misclassified,
            top_k_pairs=5,
            samples_per_pair=6
        )
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analyzer.generate_report(
            conf_matrix=results['confusion_matrix'],
            misclassified=misclassified
        )
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    results_path = os.path.join(output_dir, f'results_{args.split}.npz')
    np.savez(
        results_path,
        accuracy=results['accuracy'],
        macro_f1=results['macro_f1'],
        confusion_matrix=results['confusion_matrix'],
        per_class_precision=results['per_class_metrics']['precision'],
        per_class_recall=results['per_class_metrics']['recall'],
        per_class_f1=results['per_class_metrics']['f1'],
        per_class_accuracy=results['per_class_metrics']['accuracy'],
        predictions=results['predictions'],
        labels=results['labels']
    )
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœä¿å­˜è‡³: {output_dir}")
    print(f"\n{'='*70}")
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°')
    
    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='æ¨¡å‹checkpointè·¯å¾„'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--split',
        type=str,
        default='test',
        choices=['val', 'test'],
        help='è¯„ä¼°å“ªä¸ªæ•°æ®é›†'
    )
    
    parser.add_argument(
        '--batch_size',
        type=int,
        default=32,
        help='æ‰¹æ¬¡å¤§å°'
    )
    
    parser.add_argument(
        '--analyze_errors',
        action='store_true',
        help='æ˜¯å¦è¿›è¡Œé”™è¯¯æ ·æœ¬åˆ†æ'
    )
    
    args = parser.parse_args()
    
    main(args)