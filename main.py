"""
ä¸»è®­ç»ƒè„šæœ¬
åŠŸèƒ½: ç»Ÿä¸€çš„è®­ç»ƒå…¥å£ï¼Œæ”¯æŒä¸åŒçš„å®éªŒé…ç½®
ä½¿ç”¨æ–¹æ³•:
    python main.py --config configs/exp1_baseline.yaml
    python main.py --config configs/exp2.5_imbalance.yaml --model resnet18
"""

import os
import yaml
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import random

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data import get_dataloaders, get_train_transform, get_val_transform, get_loss_function
from models import get_model
from utils import Trainer


def set_seed(seed: int = 42):
    """è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸŒ± éšæœºç§å­è®¾ç½®ä¸º: {seed}")


def load_config(config_path: str) -> dict:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main(args):
    """ä¸»è®­ç»ƒæµç¨‹"""
    
    # åŠ è½½é…ç½®
    print(f"\n{'='*70}")
    print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    print(f"{'='*70}\n")
    
    config = load_config(args.config)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.model:
        config['model']['name'] = args.model
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    if args.epochs:
        config['training']['num_epochs'] = args.epochs
    if args.loss:
        config['loss']['type'] = args.loss
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config['seed'])
    
    # è®¾ç½®è®¾å¤‡
    device = config['device'] if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if device == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_root = config['output']['root_dir']
    exp_name = os.path.splitext(os.path.basename(args.config))[0]
    exp_output_dir = os.path.join(output_root, exp_name)
    
    model_save_dir = os.path.join(exp_output_dir, 'models')
    log_dir = os.path.join(exp_output_dir, 'logs')
    
    os.makedirs(model_save_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {exp_output_dir}\n")
    
    # å‡†å¤‡æ•°æ®å¢å¼º
    use_augmentation = config.get('use_augmentation', 'basic')
    train_transform = get_train_transform(
        image_size=config['data']['image_size'],
        use_augmentation=use_augmentation
    )
    val_transform = get_val_transform(
        image_size=config['data']['image_size']
    )
    
    # åŠ è½½æ•°æ®
    print(f"{'='*70}")
    print("ğŸ“¦ åŠ è½½æ•°æ®é›†")
    print(f"{'='*70}\n")

    # åªæœ‰åœ¨CUDAå¯ç”¨æ—¶æ‰ä½¿ç”¨pin_memory
    use_pin_memory = config['training']['pin_memory'] and torch.cuda.is_available()
    # Windowsä¸‹num_workers>0å¯èƒ½æœ‰é—®é¢˜ï¼ŒCPUæ¨¡å¼ä¸‹å»ºè®®è®¾ä¸º0
    num_workers = config['training']['num_workers'] if torch.cuda.is_available() else 0

    train_loader, val_loader, test_loader, class_to_idx = get_dataloaders(
        root_dir=config['data']['root_dir'],
        csv_file=config['data']['csv_file'],
        train_transform=train_transform,
        val_transform=val_transform,
        batch_size=config['training']['batch_size'],
        num_workers=num_workers,
        pin_memory=use_pin_memory
    )
    
    # è·å–ç±»åˆ«ç»Ÿè®¡ï¼ˆç”¨äºç±»åˆ«ä¸å¹³è¡¡å¤„ç†ï¼‰
    from data.dataset import AnimalDataset
    temp_dataset = AnimalDataset(
        root_dir=config['data']['root_dir'],
        csv_file=config['data']['csv_file'],
        split='train',
        transform=None,
        class_to_idx=class_to_idx
    )
    class_counts = temp_dataset.get_class_distribution()
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\n{'='*70}")
    print("ğŸ¤– åˆ›å»ºæ¨¡å‹")
    print(f"{'='*70}\n")
    
    model = get_model(
        model_name=config['model']['name'],
        num_classes=config['data']['num_classes'],
        pretrained=config['model']['pretrained'],
        dropout=config['model']['dropout'],
        device=device
    )
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    print(f"\n{'='*70}")
    print("ğŸ“Š åˆ›å»ºæŸå¤±å‡½æ•°")
    print(f"{'='*70}\n")
    
    criterion = get_loss_function(
        loss_type=config['loss']['type'],
        class_counts=class_counts if 'weighted' in config['loss']['type'] or 'focal' in config['loss']['type'] else None,
        num_classes=config['data']['num_classes'],
        device=device
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer_config = config['training']['optimizer']
    
    if optimizer_config['type'] == 'AdamW':
        optimizer = optim.AdamW(
            model.parameters(),
            lr=optimizer_config['lr'],
            weight_decay=optimizer_config['weight_decay']
        )
    elif optimizer_config['type'] == 'SGD':
        optimizer = optim.SGD(
            model.parameters(),
            lr=optimizer_config['lr'],
            momentum=0.9,
            weight_decay=optimizer_config['weight_decay']
        )
    else:
        raise ValueError(f"æœªçŸ¥çš„ä¼˜åŒ–å™¨: {optimizer_config['type']}")
    
    print(f"\nâœ… ä¼˜åŒ–å™¨: {optimizer_config['type']}")
    print(f"   å­¦ä¹ ç‡: {optimizer_config['lr']}")
    print(f"   æƒé‡è¡°å‡: {optimizer_config['weight_decay']}")
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_config = config['training']['scheduler']
    
    if scheduler_config['type'] == 'CosineAnnealingLR':
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=scheduler_config['T_max'],
            eta_min=scheduler_config['eta_min']
        )
    else:
        scheduler = None
    
    if scheduler:
        print(f"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_config['type']}\n")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    use_mixup = config.get('use_mixup', False)
    
    trainer = Trainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        save_dir=model_save_dir,
        use_tensorboard=config['logging']['use_tensorboard'],
        log_dir=log_dir,
        use_mixup=use_mixup,
        mixup_alpha=config.get('mixup_alpha', 1.0) if use_mixup else 1.0
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.fit(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=config['training']['num_epochs'],
        save_best_only=config['output']['save_best_only'],
        save_frequency=config['output']['save_frequency']
    )
    
    # ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬
    config_save_path = os.path.join(exp_output_dir, 'config.yaml')
    with open(config_save_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True)
    
    print(f"\nğŸ’¾ é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_save_path}")
    print(f"\n{'='*70}")
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Animal-10 å›¾åƒåˆ†ç±»è®­ç»ƒ')
    
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (ä¾‹: configs/exp1_baseline.yaml)'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default=None,
        help='æ¨¡å‹åç§° (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    
    parser.add_argument(
        '--batch_size',
        type=int,
        default=None,
        help='æ‰¹æ¬¡å¤§å° (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    
    parser.add_argument(
        '--epochs',
        type=int,
        default=None,
        help='è®­ç»ƒè½®æ•° (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    
    parser.add_argument(
        '--loss',
        type=str,
        default=None,
        help='æŸå¤±å‡½æ•°ç±»å‹ (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    
    args = parser.parse_args()
    
    main(args)